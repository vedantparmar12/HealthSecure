# üöÄ QuickStart Guide - HealthSecure RAG System

## ‚úÖ Setup Complete!

Your HealthSecure system now has **fully functional RAG** (Retrieval Augmented Generation) with Docling and Qdrant.

---

## üèÅ Start Everything

### Option 1: Auto-Start (Recommended)
```bash
start_healthsecure.bat
```
This starts:
- Backend (Go) on port 8081
- AI Service (Python) on port 5001  
- Frontend (Next.js) on port 3002

### Option 2: Manual Start
```bash
# Terminal 1 - Qdrant (if not running)
cd qdrant
qdrant.exe

# Terminal 2 - Backend
cd backend
go run cmd/server/main.go

# Terminal 3 - AI Service
cd ai-service
python app.py

# Terminal 4 - Frontend
cd frontend
npm run dev
```

---

## üß™ Test RAG System

### Quick Test
```bash
test_rag_integration.bat
```
This automated test will:
1. ‚úÖ Check all services
2. ‚úÖ Ingest sample document
3. ‚úÖ Test RAG search
4. ‚úÖ Test chat with context

### Manual Test Steps

**1. Check RAG Status:**
```bash
curl http://localhost:5000/rag/status
```

**2. Ingest Sample Document:**
```bash
cd ai-service
python ingest_with_docling.py --test-single "sample_docs/sample_medical_report.txt"
```

**3. Test Search:**
```bash
curl -X POST http://localhost:5000/rag/search ^
  -H "Content-Type: application/json" ^
  -d "{\"query\":\"blood pressure\",\"limit\":3}"
```

**4. Test Chat with RAG:**
```bash
curl -X POST http://localhost:5000/chat ^
  -H "Content-Type: application/json" ^
  -d "{\"message\":\"What are the patient's vital signs?\",\"thread_id\":\"test-001\",\"user_role\":\"doctor\"}"
```

---

## üìÑ Add Your Documents

### Single Document
```bash
cd ai-service
python ingest_with_docling.py --test-single "path/to/your/document.pdf"
```

### Batch Process
```bash
# Place PDFs in sample_docs folder, then:
python ingest_with_docling.py --documents sample_docs
```

### Via API (from anywhere)
```bash
curl -X POST http://localhost:5000/rag/upload ^
  -F "file=@C:\path\to\medical_report.pdf"
```

---

## üåê Access Your System

| Service | URL | Purpose |
|---------|-----|---------|
| **Frontend** | http://localhost:3002 | User interface |
| **Backend API** | http://localhost:8081 | Go backend |
| **AI Service** | http://localhost:5000 | RAG endpoints |
| **Qdrant Dashboard** | http://localhost:6333/dashboard | Vector DB admin |

---

## üí¨ Use RAG in Chat

### From Frontend:
1. Login to http://localhost:3002
2. Go to AI Assistant
3. Ask: "What medications is the patient taking?"
4. AI will search documents and provide sourced answers

### From API:
```bash
curl -X POST http://localhost:5000/chat ^
  -H "Content-Type: application/json" ^
  -d "{\"message\":\"Summarize the patient's condition\",\"user_role\":\"doctor\",\"user_name\":\"Dr. Smith\"}"
```

Response includes:
```json
{
  "response": "Based on the medical report...",
  "rag_sources": [
    {
      "chunk_id": "report_0",
      "similarity": 0.92,
      "source_file": "patient_report.pdf"
    }
  ],
  "rag_enabled": true
}
```

---

## üéØ What RAG Does

**Without RAG:**
```
User: "What are the patient's vital signs?"
AI: "I don't have specific patient data."
```

**With RAG:**
```
User: "What are the patient's vital signs?"
AI: "According to the medical report:
     - Blood Pressure: 145/92 mmHg (elevated)
     - Heart Rate: 88 bpm (normal range)
     - Temperature: 98.6¬∞F (37¬∞C)
     [Source: patient_report.pdf, 94% relevance]"
```

---

## üîß Configuration

### Adjust Chunk Size
Edit `ai-service/docling_rag_analyzer.py`:
```python
self.chunker = HybridChunker(
    max_tokens=512,  # Change this (256-1024)
    merge_peers=True
)
```

### Change Embedding Model
Edit `ai-service/.env`:
```env
EMBEDDING_MODEL=mxbai-embed-large  # or another Ollama model
```

### Adjust Search Results
In `app.py`, change `limit=3` to show more/fewer sources:
```python
search_results = asyncio.run(
    docling_rag.search_similar_chunks(chat_req.message, limit=5)
)
```

---

## üìä Monitor Performance

### Check Status
```bash
curl http://localhost:5000/health
```

### View Stats
```bash
curl http://localhost:5000/rag/status
```

### Qdrant Dashboard
Open http://localhost:6333/dashboard to see:
- Number of vectors stored
- Collection statistics
- Query performance
- Storage usage

---

## üêõ Common Issues

### "Qdrant not connected"
```bash
cd qdrant
qdrant.exe
```

### "No results found"
```bash
# Check if documents are indexed
curl http://localhost:5000/rag/status

# If documents_count is 0, ingest documents
cd ai-service
python ingest_with_docling.py --documents sample_docs
```

### "AI Service not running"
```bash
cd ai-service
python app.py
```

### "Import error: docling"
```bash
pip install docling docling-core qdrant-client
```

---

## üìö Documentation

- **Full Setup**: `RAG_SETUP_GUIDE.md`
- **Integration Details**: `INTEGRATION_COMPLETE.md`
- **API Documentation**: See `README.md`

---

## ‚ú® Key Features

‚úÖ **Docling**: OCR, table extraction, smart chunking
‚úÖ **Qdrant**: Fast semantic search (<100ms)
‚úÖ **Local**: All processing on your machine
‚úÖ **HIPAA**: Compliant, secure, auditable
‚úÖ **Sources**: Every answer includes sources
‚úÖ **Batch**: Process multiple documents efficiently

---

## üéâ You're Ready!

Your RAG system is **fully operational**. Start by:

1. ‚úÖ Ingest your medical documents
2. ‚úÖ Test search functionality
3. ‚úÖ Try chat with context
4. ‚úÖ Monitor with dashboards

**Need help?** Check the troubleshooting section in `RAG_SETUP_GUIDE.md`

---

**Happy coding! üöÄ**
